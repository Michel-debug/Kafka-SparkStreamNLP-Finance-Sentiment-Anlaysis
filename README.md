# 1.é¡¹ç›®æ¦‚è¿°

- ## é¡¹ç›®èƒŒæ™¯

â€‹	åœ¨å½“å‰é‡‘èå˜æ¢è«æµ‹çš„æ—¶ä»£ï¼Œå¦‚ä½•ä»é‡‘èæ–°é—»æ ‡é¢˜è·å–æ°‘ç”Ÿå¯¹è‚¡å¸‚ï¼Œç»æµï¼Œä¼ä¸šçš„çœ‹æ³•æ€åº¦å°¤ä¸ºé‡è¦ã€‚æœ¬é¡¹ç›®ä¸ºäº†åº”å¯¹å½“å‰äº’è”ç½‘**äººå·¥æ™ºèƒ½**å‘å±•è¶‹åŠ¿ä»¥åŠ**å¤§æ•°æ®**å‘å±•æ½®æµï¼Œæ­å»ºäº†ä¸€ä¸ªä»0åˆ°1çš„åŸºäº***kakfa+sparkstreaming+NLPæ·±åº¦å­¦ä¹ æ¨¡å‹+clickhouseæ•°æ®åº“***çš„***å…¨è‡ªåŠ¨å¾®æ‰¹å¼é‡‘èæ–‡æœ¬åˆ†æ***çš„é¡¹ç›®ã€‚æœ€ç»ˆå®ç°**æ•°ç§’**å†…å¯åˆ†æ**åƒæ¡**æ–°é—»å¤´æ¡æƒ…æ„Ÿä¿¡æ¯, å°†å½¢å¦‚"PayPal snaps 7-day losing streak with marginal gain"è¾“å‡ºé¢„æµ‹**positive**ã€‚ ç³»ç»Ÿæœ€ç»ˆå°†æ–°é—»æƒ…æ„Ÿåˆ†ä¸ºä¸‰å¤§ç±»positive,neutral,negative.

æ³¨æ„âš ï¸ï¼šè¯¥é¡¹ç›®ä»…ç”¨äºä¸ªäººå­¦ä¹ ï¼Œè®©åˆå­¦è€…äº†è§£å·¥ç¨‹åŒ–æŠ€æœ¯ï¼Œå®¹å™¨åŒ–æŠ€æœ¯ï¼Œç†Ÿæ‚‰å…¨æµç¨‹çš„é¡¹ç›®æ­å»ºï¼Œè¯¥é¡¹ç›®æ˜¯standaloneæ¶æ„ï¼Œæ‰€ä»¥å­¦ä¹ è€…æ— éœ€æ‹…å¿ƒåˆ†å¸ƒå¼ç¯å¢ƒï¼Œå­¦ä¹ è€…åªéœ€è¦å‡†å¤‡ä¸€å°16gå†…å­˜çš„ç¬”è®°æœ¬å°±å¥½ã€‚åç»­å°†ä¼šå‘å¸ƒåˆ†å¸ƒå¼ç‰ˆæœ¬ï¼Œè‹¥æœ‰å°ä¼™ä¼´æ„Ÿå…´è¶£ï¼Œè®°å¾—starä¸€ä¸‹å“¦â¤ï¸, å…³æ³¨ä¸è¿·è·¯ï¼ï¼

# 2.æŠ€æœ¯æ¶æ„

## æŠ€æœ¯é€‰å‹

- ç¼–è¯‘ç¯å¢ƒï¼š **vscode** + è¿œç¨‹å®¹å™¨remote æ“ä½œpythonæ–‡ä»¶ï¼ˆvscodeè¿œç¨‹remoteæ“æ§å®¹å™¨ä¸‹çš„pythonè„šæœ¬ å…·ä½“ç™¾åº¦ï¼‰
- å®¹å™¨åŒ–æŠ€æœ¯ï¼š**docker-compose** è¿›è¡ŒæœåŠ¡æ­å»º
- æ¶ˆæ¯å¤„ç†ï¼š**kafka**  æœ¬é¡¹ç›®ä½¿ç”¨å®ƒè¿›è¡Œæ•°æ®å‰Šå³°ï¼Œå‡å°‘sparkstreamingç«¯å‹åŠ›
- æ ¸å¿ƒåŠŸèƒ½ï¼š **spark** + ä½¿ç”¨æ¥è‡ª [hugging face](https://huggingface.co/) ä¸­é¢„è®­ç»ƒæ¨¡å‹**distiilbert** ( å·²åŠ è½½åˆ°æœ¬åœ° ) 
- æ•°æ®åº“ï¼š **clickhouse** åˆ—å¼å­˜å‚¨ï¼Œåœ¨æ•°æ®ç§‘å­¦é¢†åŸŸæ¡ä»¶æŸ¥è¯¢ï¼Œæœºå™¨å­¦ä¹ ï¼Œèšç±»åˆ†æï¼Œèšåˆåˆ†æï¼Œåˆ—å¼å­˜å‚¨æ•ˆç‡è¿œé«˜äºä¼ ç»Ÿè¡Œå¼å­˜å‚¨æ•°æ®åº“

## ç³»ç»Ÿæ¶æ„

![structure](./README.assets/structure.png)



1. é€šè¿‡å®šæ—¶è°ƒåº¦å™¨**crontab**ï¼ˆlinux è‡ªå¸¦ï¼Œwindowså¯é€‰ç”¨å…¶ä»–ä»»åŠ¡è°ƒåº¦å™¨ï¼‰ï¼Œè„šæœ¬ä»æ¯æ—¥æ—©ä¸Š9ç‚¹åˆ°ä¸‹åˆ5ç‚¹ æ¯éš”1å°æ—¶è°ƒåº¦ä¸€æ¬¡è·å–æœ€æ–°æ–°é—»æ•°æ®
2. æ–°é—»æ•°æ®é€šè¿‡docker ç«¯å£æ˜ å°„åˆ° **kafka**æŒ‡å®šä¸»é¢˜**finance-news**ä¸Š( kakfa_producer )ï¼Œzookeeperç”¨äºç®¡ç†kafkaï¼Œç”±äºå•æœºæ¨¡å¼ ç”¨å¤„ä¸æ˜¯å¾ˆå¤§ ä¸»è¦ç®¡ç†å…ƒæ•°æ®ã€‚
3. sparkå®¹å™¨, æœ¬é¡¹ç›®ä¸­å®¹å™¨å«**spark-sentiment**ï¼Œä½œä¸ºspark-streamingæ ¸å¿ƒï¼Œä»¥kafkaâ€”consumer çš„èº«ä»½æ¶ˆè´¹kafka broke æ•°æ®
4. spark-streaming æ¶ˆè´¹åˆ°çš„æ•°æ®å‘é€ç»™**clickhouse-server**å®¹å™¨ä¸­çš„clickhouseæ•°æ®åº“ï¼Œ
5. å®¿ä¸»æœºé€šè¿‡å¤–éƒ¨æ•°æ®åº“è¿æ¥å·¥å…·ï¼Œå¯ä»¥å®æ—¶æŸ¥çœ‹clickhouse æ•°æ®åº“å†…å®¹ï¼Œè¿›è¡Œ**OLAP**åˆ†æï¼ˆè¿™ä¸€æ­¥å¯è§†åŒ–å±•ç¤ºæ²¡æœ‰å®ç°ï¼Œæ•¢å…´è¶£çš„å°ä¼™ä¼´å¯ä»¥ç©ä¸€ä¸‹ï¼‰

ä»¥ä¸Šæ˜¯ç³»ç»Ÿæ¶æ„å…¨æµç¨‹çš„ç®€å•ä»‹ç»ï¼Œä¸‹é¢æˆ‘ä»¬æ­£å¼å¼€å§‹é¡¹ç›®æ­å»ºä»¥åŠé¡¹ç›®æ¨¡å—çš„è¯¦ç»†æ•™ç¨‹ã€‚ Allez !! c'est parti !!!!



# 3.å®‰è£…å’Œç¯å¢ƒé…ç½®(standalone)

ï¼ˆè¯´æ˜ï¼š æœ¬é¡¹ç›®æ•™ç¨‹é»˜è®¤ä½¿ç”¨ PERSON_PROJECT/ è·¯å¾„ä¸‹æ­å»ºï¼Œå°ä¼™ä¼´å¯ä»¥æŒ‰è‡ªå·±çš„æƒ³æ³•åˆ›å»ºç›®å½•ï¼‰

ï¼ˆå…·ä½“ç¯å¢ƒæ–‡ä»¶å·²ç»æ”¾åœ¨ç›®å½•ä¸­ï¼‰è‹¥ä¸¢å¤±è¯·æŒ‰ç…§ä¸‹é¢æ­¥éª¤æŒ‰ç…§

## python ç¯å¢ƒå®‰è£…ï¼ˆcondaåŒ…ç®¡ç†ï¼‰

```
conda env create -f environment.yml
conda activate nlp
```

è¯·ä½¿ç”¨æ–‡ä»¶ç›®å½•ä¸‹çš„enviroment.yml è¿›è¡Œç¯å¢ƒç»Ÿä¸€é…ç½®

##### é¢„å¤‡æ¡ä»¶ï¼š è‹¥æ²¡æœ‰å®‰è£…ï¼Œè¯·å…ˆå»å®˜ç½‘å®‰è£…[dockerå®¢æˆ·ç«¯](https://www.docker.com)

## Mac armæ¶æ„ï¼ˆmèŠ¯ç‰‡ï¼‰

å¯¹äºmacå°ä¼™ä¼´ï¼Œè€å¸ˆå·²ç»ç»™ä½ ä»¬å‡†å¤‡å¥½äº†docker é•œåƒç¯å¢ƒï¼Œ


### clickhouse,spark, kafka ä»¥åŠ zookeeperï¼Œä½¿ç”¨docker-compose å®‰è£…

åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»ºdocker-compose.yml, ç¼–è¾‘è¯¥æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹

```yml
version: "3.0"
networks:
  myNetwork:

services:	
  clickhouse:
    image: 'chen0177/clickhouse:latest'
    container_name: clickhouse-server
    restart: no
    ports:
    	- '8123':'8123'
    volumes:
    	- ./storage/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml
    	- ./storage/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml
    	- ./storage/clickhouse/data:/var/lib/clickhouse/
    networks:
      - myNetwork
	spark:
		image: 'chen0177/pyspark:latest'
    container_name: spark_sentiment
    restart: no
    ports:
      - '8888:8888'
      - '4040:4040'
    networks:
      - myNetwork
      
  zookeeper:
    image: 'chen0177/zookeeper:latest'
    container_name: zookeeper
    restart: no
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper 
    networks:
      - myNetwork
      
  kafka:
    image: 'chen0177/kafka:latest'
    container_name: kafka
    user: root
    restart: no
    ports:
      - '9092:9092'
      - '9093:9092'
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      # 9093 ç»™å†…ç½‘ä½¿ç”¨ sparkå®¹å™¨è¿æ¥ èµ°è¿™æ¡è·¯ï¼Œ 9092ç»™å¤–éƒ¨pythonè„šæœ¬ä½¿ç”¨
      - KAFKA_LISTENERS=INTERNAL://:9093,EXTERNAL://:9092
      # spark å®¹å™¨æ ¹æ®å®¹å™¨é€šä¿¡è¿æ¥9092ï¼Œ å®¿ä¸»æœºæ ¹æ®ç«¯å£æ˜ å°„åŸç†è¿æ¥9092
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9092
      # ä½¿ç”¨ä¸åŒçš„ç›‘å¬å™¨åç§°æ¥åŒºåˆ†ä¸åŒçš„å®‰å…¨å’Œç½‘ç»œé…ç½®
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes

    volumes:
      - ./Kafka:/bitnami/kafka
    networks:
      - myNetwork
    depends_on:
      - zookeeper
      

```

â€‹	æœ€åä¿å­˜æ–‡ä»¶ï¼Œå‡è®¾åœ¨è¯¥ï¼ˆåœ¨person_project/ï¼‰ç›®å½•ä¸‹ï¼Œæ‰§è¡Œ `docker-compose up -d`ï¼Œå³å¯å®Œæˆæ­å»º. å‘½ä»¤è¡Œç»ˆç«¯è¾“å…¥`docker ps -a`ç°åœ¨çš„docker å®¹å™¨åº”è¯¥å‘å¦‚ä¸‹æ‰€ç¤º


![container](./README.assets/container.png)

**ï¼ˆæ³¨æ„è‹¥ä»¥ä¸Šç¯å¢ƒdocker-compose å­˜åœ¨é—®é¢˜), è¯·åˆ†å¼€ä½¿ç”¨docker å•ç‹¬åŠ è½½é•œåƒã€‚ç›´æ¥å»å®˜ç½‘æ‹‰å–æœ€æ–°çš„é•œåƒå®‰è£…å³å¯**

## windows ç¯å¢ƒ 

### clickhouse å®‰è£…è¯´æ˜

å¯¹äºwindows çš„å°ä¼™ä¼´ï¼ŒåŒæ ·å¯ä»¥ä½¿ç”¨docker-compose å®‰è£…ä½†æ˜¯å”¯ä¸€è¦æ³¨æ„çš„çš„æ˜¯clickhouseçš„å®‰è£…ï¼Œç”±äºmac ç‰ˆæœ¬è€å¸ˆå·²ç»åœ¨é•œåƒä¸­åšäº†é…ç½®æ–‡ä»¶çš„æ›´æ”¹ï¼Œwindowså¹¶æ²¡æœ‰æ›´æ”¹é…ç½® å› æ­¤éœ€è¦è¿›è¡Œä¸‹é¢é…ç½®ï¼Œé¦–å…ˆå•ç‹¬å®‰è£…clickhouse å®¹å™¨ï¼Œæ“ä½œå¦‚ä¸‹ï¼š

```shell
############ clickhouse docker install
docker run --rm -d --name=temp-clickhouse-server clickhouse/clickhouse-server:latest

# in local machine, create these three directories, after we need to use it as volume
# attention: you should choose the right path that you preferred  in your local machine
mkdir -p ./storage/clickhouse/conf ./storage/clickhouse/data ./storage/clickhouse/log

docker cp temp-clickhouse-server:/etc/clickhouse-server/users.xml ./storage/clickhouse/conf/users.xml
docker cp temp-clickhouse-server:/etc/clickhouse-server/config.xml ./storage/clickhouse/conf/config.xml

docker stop temp-clickhouse-server

```

æ¥ç€åœ¨**æœ¬åœ°** ç›®å½•æ‰“å¼€åˆšæ‰æˆ‘ä»¬æŒ‚è½½çš„å·è½´ ./storage/clickhouse/conf/users.xmlï¼Œä¿®æ”¹æ–‡ä»¶å¦‚ä¸‹
![clickhouse_listen](./README.assets/clickhouse_listen.png)

å›¾è§£ï¼š æ›´æ”¹æˆä½ éœ€è¦ç›‘å¬**(æ–‡ä»¶æŸ¥æ‰¾listen)**çš„ç«¯å£å·ï¼Œå½“å‰æˆ‘ä»¬åœ¨å­¦ä¹ é˜¶æ®µï¼Œå•æœºæ¨¡å¼ï¼Œå†™æˆä»»æ„å³å¯ï¼Œç”Ÿäº§ç¯å¢ƒä¸€å®šè¦ä¿®æ”¹å“¦ï¼



æ‰“å¼€ user.xml,ä¿®æ”¹å¦‚ä¸‹
![clickhouse_user](./README.assets/clickhouse_user.png)

å›¾è§£ï¼šç»™clickhouse è®¾ç½®åˆå§‹å¯†ç **(æ–‡ä»¶æŸ¥æ‰¾password)**

æ¥ç€åœ¨å½“å‰é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œç»ˆç«¯æ‰§è¡Œä¸‹é¢å‘½ä»¤

```shell
docker run -d --name=clickhouse-server \
-p 8123:8123 \
--network=person_project_myNetwork \
--volume=./storage/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml \
--volume=./storage/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml \
--volume=./storage/clickhouse/data:/var/lib/clickhouse/ \
clickhouse/clickhouse-server:latest
```

è¿™æ ·clickhouse å®¹å™¨å°±å®‰è£…å¥½å•¦ã€‚

### sparkï¼Œkafkaï¼Œzookeeper å®‰è£…

åŒç†è¿˜æ˜¯ä½¿ç”¨docker-compose æ„å»ºï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ–°å»ºæ–‡ä»¶docker-compose.xmlï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯æ›´æ¢imageæº

```yml
version: "3.0"
networks:
  myNetwork:

services:	
	spark:
		image: 'jupyter/pyspark-notebook'
    container_name: spark_sentiment
    restart: no
    ports:
      - '8888:8888'
      - '4040:4040'
    networks:
      - myNetwork
      
  zookeeper:
    image: 'bitnami/zookeeper:latest'
    container_name: zookeeper
    restart: no
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper 
    networks:
      - myNetwork
      
  kafka:
    image: 'bitnami//kafka:latest'
    container_name: kafka
    user: root
    restart: no
    ports:
      - '9092:9092'
      - '9093:9092'
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
      # 9093 ç»™å†…ç½‘ä½¿ç”¨ sparkå®¹å™¨è¿æ¥ èµ°è¿™æ¡è·¯ï¼Œ 9092ç»™å¤–éƒ¨pythonè„šæœ¬ä½¿ç”¨
      - KAFKA_LISTENERS=INTERNAL://:9093,EXTERNAL://:9092
      # spark å®¹å™¨æ ¹æ®å®¹å™¨é€šä¿¡è¿æ¥9092ï¼Œ å®¿ä¸»æœºæ ¹æ®ç«¯å£æ˜ å°„åŸç†è¿æ¥9092
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9092
      # ä½¿ç”¨ä¸åŒçš„ç›‘å¬å™¨åç§°æ¥åŒºåˆ†ä¸åŒçš„å®‰å…¨å’Œç½‘ç»œé…ç½®
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes

    volumes:
      - ./Kafka:/bitnami/kafka
    networks:
      - myNetwork
    depends_on:
      - zookeeper
      

```



### å®¹å™¨é€šä¿¡æµ‹è¯•

ä½¿ç”¨å‘½ä»¤`docker network ls` å°†ä¼šæ˜¾ç¤ºä½ çš„docker å®¹å™¨ç½‘ç»œä¿¡æ¯ 
![docker_network](./README.assets/docker_network.png)

ä½¿ç”¨`docker network inspect person_project_myNetwork`, åº”è¯¥ä¼šå‡ºç°ä¸‹é¢çš„ä¿¡æ¯
![docker_inspect](./README.assets/docker_inspect.png)

ä½ ä¼šå‘ç°ä»–ä»¬åœ¨åŒä¸€ä¸ªå±€åŸŸç½‘ä¸‹ï¼Œç°åœ¨ä½ å¯ä»¥å¼€å§‹è¿›å…¥ä¸€ä¸ªå®¹å™¨ï¼Œæ¯”å¦‚`docker exec -it spark_sentiment bash`, é€šè¿‡ping å®¹å™¨åç§°çš„å½¢å¼ï¼ŒæŸ¥çœ‹å®¹å™¨é—´é€šä¿¡æ˜¯å¦æ­£å¸¸. æŸ¥çœ‹æ˜¯å¦ä¸¢åŒ…
ä½¿ç”¨ `nc -vz å®¹å™¨åç§°:ç«¯å£å·` ï¼ŒæŸ¥çœ‹å®¹å™¨é—´ç«¯å£å·è®¿é—®æ˜¯å¦æ­£å¸¸. å¦‚ä½•æˆåŠŸç›‘å¬å°†ä¼šè¿”å›**succeed** 

åˆ°è¿™é‡Œæˆ‘ä»¬å·²ç»å®Œæˆäº†å…¨éƒ¨å®¹å™¨é€šä¿¡çš„éƒ¨ç½²ï¼Œå¦‚æœè¿˜å­˜åœ¨é—®é¢˜è¯·**æŸ¥çœ‹æ–‡æ¡£å¸¸è§é—®é¢˜**éƒ¨åˆ†ã€‚

åŠ æ²¹ä½ ç¦»æˆåŠŸä¸è¿œäº†ã€‚ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘



# 4.æ­å»ºé¢„å¤„ç†

## 4.1 kafka topicæ­å»º

è¿è¡Œkafka å®¹å™¨ï¼Œ`docker exec -it kafka bash`, è¿›å…¥å®¹å™¨ä»¥åæ‰§è¡Œä¸‹é¢å‘½ä»¤ åˆ›å»ºkafka topicï¼Œç”±äºæœ¬é¡¹ç›®æ˜¯å•æœºçš„ï¼Œä¸è€ƒè™‘åˆ†å¸ƒå¼çš„æƒ…å†µï¼Œå› æ­¤æ¯”è¾ƒç®€å•ï¼Œé‚£ä¹ˆåç»­åˆ†å¸ƒå¼é¡¹ç›®ä¹Ÿå°†ä¼šå‘å‡º.æ•¬è¯·æœŸå¾…

```shell
# åˆ›å»ºåä¸º finance-news çš„ä¸»é¢˜ï¼Œå¹¶è®¾ç½®ä¿ç•™ç­–ç•¥ä¸ºä¸€å¤©
kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic finance-news --config retention.ms=86400000

# æŸ¥çœ‹å½“å‰ä¸»é¢˜çš„é…ç½®ï¼š ä½¿ç”¨ kafka-configs.sh å·¥å…·æŸ¥çœ‹å½“å‰ä¸»é¢˜çš„é…ç½®ï¼Œä»¥äº†è§£å½“å‰çš„ä¿ç•™ç­–ç•¥å’Œå…¶ä»–é…ç½®ã€‚
# kafka-configs.sh --bootstrap-server localhost:9092  --entity-type topics --describe --entity-name finance-news

# ä¿®æ”¹ä¿ç•™ç­–ç•¥ï¼š ä½¿ç”¨ kafka-configs.sh å·¥å…·æ¥ä¿®æ”¹ä¸»é¢˜çš„ä¿ç•™ç­–ç•¥ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ --alter å‚æ•°æ¥æŒ‡å®šè¦ä¿®æ”¹çš„é…ç½®ã€‚
# kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --alter --entity-name finance-news --add-config retention.ms=86400000

```

æˆåŠŸåˆ›å»ºå¥½åå¯ä»¥æ‰“å¼€ä¸¤ä¸ªdocker kafkaå®¹å™¨ç»ˆç«¯ ä½¿ç”¨producerï¼Œconsumer åœ¨æœ¬åœ°æµ‹è¯•æ˜¯å¦å¯ä»¥ç”Ÿå­˜å’Œæ¶ˆè´¹æˆåŠŸ
`kafka-console-producer.py --bootstrap-server localhost:9092 --topic finance-news`
`kafka-console-consumer.py --bootstrap-server localhost:9092 --topic finance-news`

## 4.2 clickhouseå»ºè¡¨

è¿è¡Œclickhouseå®¹å™¨ï¼Œç”±äºæˆ‘ä»¬çš„é¡¹ç›®ä»…ä¾›å­¦ä¹ ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„æ•°æ®åº“defaultï¼Œè¯·å°ä¼™ä¼´ä»¬ä½¿ç”¨navicatï¼Œæˆ–è€…datagrip å®¢æˆ·ç«¯è¿›è¡Œæ•°æ®åº“è¿æ¥ï¼Œè¿æ¥æ¯”å¦‚å¦‚ä¸‹æ‰€ç¤º![clickhouse_choose](./README.assets/clickhouse_choose.png)


![clickhouse_password](./README.assets/clickhouse_password.png)

æ³¨æ„ç¬¬ä¸€æ¬¡è¿è¡Œçš„å°ä¼™ä¼´ï¼šå¯èƒ½éœ€è¦ä¸‹è½½é©±åŠ¨æ–‡ä»¶ï¼Œç­‰å¾…å‡ åˆ†é’Ÿä¸‹è½½å®Œå³å¯ï¼Œè¾“å…¥æœ¬åœ°å¯†ç  ä¹‹å‰é…ç½®æ–‡ä»¶é…è¿‡çš„ï¼Œè¿æ¥
å¦‚æœåˆ°è¿™é‡ŒæˆåŠŸäº†ï¼Œé‚£ä¹ˆæ­å–œä½ ï¼Œæˆ‘ä»¬é¡¹ç›®æ­å»ºåŸºæœ¬å¿«å®Œæˆäº†ï¼Œåªå·®æœ€åä¸€æ­¥ï¼Œå»ºè¡¨

```sql
## DDL TABLE 
## consider our data is not big, so we divide the data by month
CREATE TABLE news_analysis (
    news_text String,
    prediction_result String,
    timestamp DateTime
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY timestamp;
```

è§£é‡Šï¼š åœ¨è¿™é‡Œæˆ‘ä»¬é€‰ç”¨æŒ‰æ¯æ—¥åˆ†åŒºï¼Œå› ä¸ºä¸€å¤©å¤§æ¦‚ä¼šè½½å…¥16æ‰¹æ•°æ®ï¼Œæ¯æ‰¹æ•°æ®æ ¹æ®apiè¯·æ±‚çš„ä¸ªæ•°æˆç™¾ä¸Šåƒä¸ç­‰ï¼Œä¸€å¤©ä¸‹æ¥å¯èƒ½æœ‰ä¸‡æ¡æ•°æ®ï¼Œæ‰€ä»¥æŒ‰å¤©åˆ†åŒºæ˜¯ä¸ªå¥½æ‰‹æ®µã€‚ 
ä»‹ç»ä¸€ä¸‹ç¬¬ä¸€ä¸ªå­—æ®µï¼Œç¬¬ä¸€ä¸ªå­—æ®µç”¨äºå­˜å‚¨æ–°é—»æ–‡æœ¬ï¼Œç¬¬äºŒä¸ªå­—æ®µç”¨äºé¢„æµ‹æ–°é—»æ–‡æœ¬ï¼Œç¬¬ä¸‰ä¸ªå­—æ®µç”¨äºå­˜å‚¨å½“æ—¥è½½å…¥æ—¶é—´ï¼Œ å…·ä½“éœ€æ±‚å¯ä»¥æ ¹æ®ä½ ä»¬ä»¥åè‡ªå·±çš„é¡¹ç›®æ¥æ‰©å±•ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€å§‹ã€‚



## 4.3 sparkå®¹å™¨ä¸‹è„šæœ¬æ–‡ä»¶åˆ›å»º

åœ¨spark-sentiment å®¹å™¨ å®¶ç›®å½•ä¸‹æ–°å»ºkf_to_sstreamingçš„è„šæœ¬æ–‡ä»¶

![docker_script_intro](./README.assets/docker_script_intro.png)

æ­£å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œsystemeæ˜¯æˆ‘ä»¬è‡ªå·±æ“ä½œç³»ç»Ÿä¸‹çš„è„šæœ¬æ–‡ä»¶ï¼Œç„¶è€ŒsparkstreamingæœåŠ¡åœ¨å®¹å™¨spark-sentimentä¸­ï¼Œå› æ­¤æˆ‘ä»¬æƒ³è¦ä½¿ç”¨sparkç¯å¢ƒåªèƒ½åœ¨sparkå®¹å™¨ä¸­è¿›è¡Œï¼Œæˆ‘ä»¬éœ€è¦åœ¨sparkå®¹å™¨ æ–°å»ºkf_to_sstreaming.py ä¸ºåç»­ç¨‹åºè¿è¡Œåšé“ºå«ã€‚

åˆ°è¿™é‡Œæˆ‘ä»¬çš„å‰æœŸå‡†å¤‡å¿«è¦åŠ é€Ÿäº†ï¼Œå¦‚æœä½ èƒ½å­¦åˆ°è¿™é‡Œï¼Œé‚£ä½ çœŸçš„å¾ˆä¸é”™ï¼ŒåŠ æ²¹â›½ï¸â›½ï¸

# 5.æ­å»ºä¸»è¦åŠŸèƒ½å’Œæ¨¡å—



## 5.1 api æ–°é—»å¾®æ‰¹å¼æŠ“å–

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»ºä¸€ä¸ªapi_news.py æ–‡ä»¶, è¯¥æ–‡ä»¶ä¸»è¦ç”¨äºå­˜å‚¨apiå‡½æ•°ï¼Œè¯¥å‡½æ•°ä¸»è¦è¿”å›jsonç»“æ„ï¼Œåœ¨åç»­ä¸­æˆ‘ä»¬åªè¦æ–°é—»çš„å¤´æ¡æ–‡æœ¬
å¦‚ä¸‹ï¼ˆéƒ¨åˆ†ï¼‰ï¼š

```python
from newsapi import NewsApiClient
import requests
def fetch_financial_news_seekingalpha():
    
    url = "https://seeking-alpha.p.rapidapi.com/news/v2/list"

    querystring = {"category":"market-news::financials","size":"40"}
    
    headers = {
        "X-RapidAPI-Key": "e3ffde7ad0msh33323e10f821f13p15885bjsn25fc84293c2a",
        "X-RapidAPI-Host": "seeking-alpha.p.rapidapi.com"
    }

    response = requests.get(url, headers=headers, params=querystring)

    return response.json()
```



## 5.2 æ¨é€åˆ°kafka topicæµå‡½æ•°

åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºåä¸ºkafka_connector.py æ–‡ä»¶ï¼Œå¯¹äºæ¯ä¸ªapiå‡½æ•°æˆ‘ä»¬éƒ½å‘é€ç»™kafkaå¤„ç†ï¼Œå¦‚ä¸‹ï¼ˆéƒ¨åˆ†ï¼‰ï¼Œå®Œæ•´ä»£ç è¯·çœ‹githubæ–‡ä»¶

```python
from kafka import KafkaProducer
import json
import re
import api_News

# Clean data rules
pattern = re.compile(r'^[a-zA-Z0-9\s.,!?\'"()-]*$')

# kafka's configuration
kafka_server = 'localhost:9093'
topic_name = 'finance-news'

# create Kafka producerï¼Œmessage code as UTF-8
producer = KafkaProducer(bootstrap_servers=kafka_server,
                         value_serializer=lambda m: json.dumps(m).encode('utf-8'))

# send news to kafka fonction
def send_news_to_kafka(headline):
    try:
        future = producer.send(topic_name, value=headline)
        result = future.get(timeout=10)
        print(f"Sent and acknowledged: {headline} at offset {result.offset}")
    except Exception as e:
        print(f"Failed to send headline to Kafka: {headline}, Error: {e}")
    

# excute the fetch and send news
news_apis = [
    (api_News.fetch_bbc_news, "BBC"),
    (api_News.fetch_financial_news_seekingalpha, "SeekingAlpha"),
    (api_News.fetch_financial_news_newsdataio, "NewsdataIO"),
    (api_News.fetch_Yahoo_financial_news_famous_brand, "Yahoo"),
    (api_News.fetch_financial_news_apple, "Apple"),
    (api_News.fetch_financial_news_tsla2, "Tesla"),
    (api_News.fetch_news_everything, "Everything")
]

def fetch_and_send_news(api_method, api_name):
    try:
        sources_news = api_method()
        if api_name=="Yahoo":
            for source in sources_news:
                for i in source.keys():
                # try to send the news to kafka, ifnot exception
                    send_news_to_kafka(source[i]['title'])

-ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚

```



## 5.3 sparkstreaming + NLPå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

åœ¨docker spark_sentiment å®¹å™¨ä¸­å®¶ç›®å½•ä¸‹æ–°å»ºpythonæ–‡ä»¶ï¼Œåå­—å«kf_to_sstreaming.py ç›®å½•

å°†huggingface æ¨¡å‹ï¼ˆè§é™„å½•é“¾æ¥ï¼‰ä¸‹è½½åˆ°å®¹å™¨ä¸­ä½¿ç”¨ï¼Œå…ˆä¸‹è½½åˆ°å®¿ä¸»æœºï¼Œç„¶åä½¿ç”¨
`docker cp [æ¨¡å‹åç§°]  spark_sentiment:/home/joyvan/`  æ¨é€åˆ°å®¹å™¨çš„å®¶ç›®å½•ä¸‹å’Œè„šæœ¬æ–‡ä»¶åŒç›®å½•

```python
ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
# åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
model_path = './distilbert-base-uncased_System_analyse_sentiment'
model = AutoModelForSequenceClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)
# ä½¿ç”¨pipelineç®€åŒ–é¢„æµ‹è¿‡ç¨‹
classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)
## mac m1/m2/m3 ä½¿ç”¨ mps , windows ä½¿ç”¨ cuda
device = torch.device("mps" if torch.backends.mps.is_available() else "cuda")
model.eval()  # Set model to evaluation mode

def predict_sentiment(headlines):
    # Prepare input
    inputs = tokenizer(headlines, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=-1)
    predicted_class_index = probabilities.argmax(dim=-1).item()
    
    # Mapping class index to label
    sentiment_labels = {0: "negative", 1: "neutral", 2: "positive"}
    return sentiment_labels[predicted_class_index]
 ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚ã€‚
```

åœ¨è¿™é‡Œæœ€ç»ˆå®ç°çš„åŠŸèƒ½æ˜¯ï¼Œä¼ å…¥ä¸€è¡Œæ•°æ®ï¼Œè¾“å‡ºä¸€ä¸ªæƒ…æ„Ÿåˆ†ç±»


## 5.4 sparkstreaming è¯»å–kafkaæº

é…ç½®kafka æœåŠ¡å™¨åœ°å€å’Œtopic ï¼Œä»£ç å¦‚ä¸‹ï¼ˆéƒ¨åˆ†ï¼Œæ¥ç€ä¸Šé¢çš„è„šæœ¬æ–‡ä»¶ï¼‰ï¼Œå…·ä½“æŸ¥çœ‹github

```python
# åˆå§‹åŒ– Spark ä¼šè¯
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .config("spark.streaming.backpressure.enabled", "true") \
    .config("spark.streaming.batchDuration", "10") \
    .getOrCreate()

# æœ¬åœ°èµ„æºä¸å¤Ÿï¼Œå…³é—­æ£€æŸ¥ç‚¹
spark.conf.set("spark.sql.streaming.forceDeleteTempCheckpointLocation", "true")
spark.sparkContext.setLogLevel("WARN")
# è®¾ç½® Kafka æœåŠ¡å™¨å’Œä¸»é¢˜
kafka_topic_name = "finance-news"
kafka_bootstrap_servers = 'kafka:9093'

# è¿™é‡Œä½¿ç”¨timeoutå‚æ•°ï¼Œé¿å…å› ä¸ºç½‘ç»œé—®é¢˜å¯¼è‡´çš„è¶…æ—¶é—®é¢˜ï¼Œ ä»¥åŠåœ¨ä½¿ç”¨æ—¶å‡ºç° warnè­¦å‘Š kafka-1894, ä¸è¦æ‹…å¿ƒï¼Œpysparkä¸kafkaå…¼å®¹æ€§é—®é¢˜
# å¦‚æœä¸å–œæ¬¢warnï¼Œä¿®æ”¹log4j.propertiesæ–‡ä»¶ï¼Œå°†log4j.logger.org.apache.kafka.clients.NetworkClient=ERRORï¼Œè‡ªè¡Œgoogleæœç´¢
df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
  .option("subscribe", kafka_topic_name) \
  .option("startingOffsets", "latest") \
  .load()
```



## 5.5 sparkstreaming å†™å…¥clickhouseæ•°æ®åº“

è¿˜æ˜¯æ¥ç€ä¸Šé—¨è„šæœ¬æ–‡ä»¶ï¼Œå…¨æ–‡ä»¶æˆ‘æ”¾åœ¨äº†githubä¸Š

```python
## ================== æ•°æ®å¤„ç† ================================================
## åˆ—åéœ€è¦ä¸ clickhouse æ•°æ®åº“è¡¨å­—æ®µåä¸€è‡´
# è½¬æ¢ key å’Œ value ä»äºŒè¿›åˆ¶åˆ°å­—ç¬¦ä¸²
df_transformed = df.select(
    #col("key").cast("string").alias("key"),  ç›®å‰ä¸éœ€è¦ keyï¼Œå¯è‡ªè¡Œæ·»åŠ 
    col("value").cast("string").alias("news_text")
)
# æ³¨å†Œ UDF
predict_sentiment_udf = udf(predict_sentiment, StringType())
# åº”ç”¨ NLP æ¨¡å‹è¿›è¡Œåˆ†æ
df_transformed = df_transformed.withColumn("prediction_result", predict_sentiment_udf(col("news_text")))  # ä¼ å…¥ value åˆ—
# åŠ å…¥å½“å¤©æ—¥æœŸ
df_transformed = df_transformed.withColumn("timestamp",date_format(current_timestamp(),"yyyy-MM-dd HH:mm:ss"))

# å®šä¹‰ ClickHouse çš„è¿æ¥å‚æ•°
url = "jdbc:clickhouse://clickhouse-server:8123/default"
properties = {
    "user": "default",   # é»˜è®¤ç”¨æˆ·å
    "password": "123456",      # é»˜è®¤æ²¡æœ‰å¯†ç ï¼Œå¦‚æœè®¾ç½®äº†å¯†ç éœ€è¦æ›´æ”¹
    "driver": "com.clickhouse.jdbc.ClickHouseDriver"
}


# é…ç½®å†™å…¥ ClickHouse çš„æ•°æ®æµ,ä½†æ˜¯ clickhouse ä¸æ”¯æŒdataframeçš„æµå¼å†™å…¥ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨foreachBatch å¾®æ‰¹æ¬¡æµå†™å…¥
# epoch_id æ˜¯æ¯ä¸ªæ‰¹æ¬¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ å¿…é¡»å†™å…¥ï¼Œå¦åˆ™ä¼šæŠ¥é”™
def write_to_clickhouse(batch_df, epoch_id):
  try:
    """
    å†™å…¥æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®åˆ° ClickHouseã€‚
    """
    batch_df.write \
        .format("jdbc") \
        .option("url", url) \
        .option("dbtable", "news_analysis") \
        .option("user", properties["user"]) \
        .option("password", properties["password"]) \
        .option("driver", properties["driver"]) \
        .option("checkpointLocation", "./checkpoint/analysis") \
        .mode("append") \
        .save()
    print(f"Batch {epoch_id} successfully written to ClickHouse.")
  except Exception as e:
    print(f"Error writing batch {epoch_id} to ClickHouse: {str(e)}")


query = df_transformed.writeStream \
    .outputMode("append") \
    .foreachBatch(write_to_clickhouse) \
    .start()

query.awaitTermination()

```

## 5.6 å®¿ä¸»æœºæŸ¥è¯¢clickhouseæ•°æ®åº“æ•°æ®

å½“æˆ‘ä»¬å¯åŠ¨dockerå®¹å™¨ clickhouse-server æ—¶ï¼Œé»˜è®¤æ•°æ®åº“æœåŠ¡å·²ç»å¯åŠ¨ï¼Œå¯ä»¥åœ¨å®¿ä¸»æœºä¸Šä½¿ç”¨navicatæˆ–è€…datagripç­‰å®¢æˆ·ç«¯å·¥å…·è¿›è¡ŒæŸ¥è¯¢ï¼Œè¿™é‡Œæˆ‘ä»¥datagripå®¢æˆ·ç«¯ä¸ºä¾‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º![clickhoust_connect2](./README.assets/clickhoust_connect2.png)

ç›´æ¥æ‰¾åˆ°å¯¹åº”çš„æ•°æ®åº“ï¼Œè¾“å…¥åˆå§‹åŒ–å¯†ç ï¼Œåœ¨æœ€åˆé…ç½®æ–‡ä»¶ä¸­æ˜¯123456ï¼Œè¾“å…¥è¿æ¥å³å¯ï¼Œæœ€ç»ˆæ•°æ®åº“ä¼šæ˜¾ç¤ºå¦‚ä¸‹æ‰€ç¤ºï¼ˆä»¥ä¸‹ç»“æœä»…ç”¨äºå±•ç¤ºï¼Œé¡¹ç›®å¯åŠ¨è¯·çœ‹ä¸‹é¢ä¸€ç« å†…å®¹ï¼‰![clickhouse_requery](./README.assets/clickhouse_requery.png)

è‡³æ­¤é¡¹ç›®ï¼Œå…¨æ¨¡å—å·²ç»æ­å»ºå®Œæˆï¼Œä¸‹ä¸€ç« æˆ‘å°†å¸¦é¢†å¤§å®¶å¦‚ä½•å¯åŠ¨æˆ‘ä»¬çš„é¡¹ç›®ã€‚

# 6.ç”¨æˆ·æ‰‹å†Œ

## **github é¡¹ç›®æ–‡ä»¶ä»‹ç»**

- **kf_to_sstreaming.pyï¼šè¯·æŠŠè¯¥æ–‡ä»¶æ”¾å…¥spark-sentimentå®¹å™¨çš„å®¶ç›®å½•ä¸‹**

- api_test.ipynb: æ—©æœŸæµ‹è¯•apiä½¿ç”¨ï¼Œæ— æ„ä¹‰

- **api_News.py: apiæ–°é—»è°ƒç”¨æ¨¡å—ï¼Œå°è£…å„ä¸ªæ–°é—»apiå‡½æ•°**

- docker-compose.xml: æ—©æœŸdocker-composeé…ç½®æ–‡ä»¶ï¼Œ**å¤§å®¶æ ¹æ®æ•™ç¨‹ä¸Šçš„æ–‡ä»¶è¿›è¡Œæ›´æ–°ï¼Œä¸è¦ä½¿ç”¨æˆ‘çš„é…ç½®æ–‡ä»¶ï¼**

- start_server.sh é¡¹ç›®å¯åŠ¨è„šæœ¬ï¼Œä»…ä»…å¯åŠ¨å®¹å™¨

- stop_server.sh é¡¹ç›®å…³é—­è„šæœ¬ï¼Œï¼ˆè¿™ä¸ªå’Œä¹‹å‰è¿™ä¸ªæœ‰bugæœªä¼˜åŒ–ï¼Œå°ä¼™ä¼´å¯ä»¥è‡ªå·±è®¾è®¡è„šæœ¬ ä¸éš¾ï¼‰

- **kafka_connector.py: é‡‡é›†æ–°é—»æ–‡æœ¬åˆ°kafka broke ä¸­**

- storageï¼š clickhouse å®¹å™¨æŒ‚è½½æ–‡ä»¶ç›®å½•

  ï¼ˆæœ¬é¡¹ç›®æ ¸å¿ƒæ–‡ä»¶ä¸ºåŠ ç²—å­—ä½“ï¼‰

## è°ƒåº¦å·¥å…·

linux/Mac ç³»ç»Ÿ ä½¿ç”¨crontab å¯¹ kafka_connector.py è¿›è¡ŒåŠå°æ—¶è°ƒåº¦
è¾“å…¥å‘½ä»¤ crontab -e æ­¤æ—¶åœ¨æ‰“å¼€çš„æ–‡ä»¶ä¸­è¾“å…¥ä»¥ä¸‹å‘½ä»¤

```
0 9-17 * * * /Users/chenchenjunjie/anaconda3/envs/nlp/bin/python ./Person_project/kafka_connector.py >> ./Person_project/logfile.log 2>&1

```

æ­¤æ—¶è¯¥è„šæœ¬å¯ä»¥æ­£å¸¸æ ¹æ®ç³»ç»Ÿæ—¶é—´è¢«è°ƒç”¨, æ—¥å¿—æ–‡ä»¶ä¼šè¾“å‡ºç¨‹åºè¿è¡Œç»“æœ

windowsç³»ç»Ÿï¼Œå¯è‡ªè¡Œåœ¨ç½‘ä¸Šæœç´¢ä»»åŠ¡è°ƒåº¦å·¥å…·å³å¯

å¦å¤–ï¼Œæœ¬é¡¹ç›®è‡ªåŠ¨åŒ–ä»»åŠ¡ä¸æ˜¯å¾ˆå¤šï¼Œè‹¥æ­å»ºç¹æ‚ä»»åŠ¡å¤šçš„è‡ªåŠ¨åŒ–é¡¹ç›®ï¼Œå¯ä»¥ä½¿ç”¨airflowï¼Œdolphin scheduleç­‰ä¼˜ç§€è°ƒåº¦ä»»åŠ¡å·¥å…·ã€‚

## é¡¹ç›®ç¨‹åºæ‰§è¡Œé¡ºåºä»¥åŠè¿è¡Œä»£ç 

1. ä¾æ¬¡å¼€å¯docker å®¹å™¨ï¼Œ zookeeperï¼Œ clickhouse-serverï¼Œ kafkaï¼Œspark-sentiment

2. åœ¨spark-sentimentå®¹å™¨ä¸­ï¼Œè½½å…¥**kf_to_sstreaming.py**è„šæœ¬, åœ¨å®¹å™¨ç»ˆç«¯è¿è¡Œ

   ```shell
   ## åœ¨æäº¤æ—¶å¯èƒ½è¿˜ä¼šå‡ºç°ä¼šhttp5client çš„è­¦å‘Šï¼Œå¯¼å…¥mavenåŒ…å°±å¥½
   spark-submit \
   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.6.0,org.apache.httpcomponents.client5:httpclient5:5.3.1 \
   kf_to_sstreaming.py 
   ```

â€‹     ![spark_succed](./README.assets/spark_succed.png)

3. åœ¨æœ¬åœ°å®¿ä¸»æœºé¡¹ç›®ç›®å½•ä¸‹ ç»ˆç«¯è¿è¡Œ `python3  kafka_connector.py`ï¼Œæˆ–è€…å¦‚æœé…ç½®crontab ï¼Œå®ƒå°†ä¼šè‡ªåŠ¨æ‰§è¡Œã€‚

![kafka_connector](./README.assets/kafka_connector.png)

è¿è¡Œå®Œè¯¥è„šæœ¬æ–‡ä»¶ï¼Œç­‰å¾…æ­¥éª¤2ç»ˆç«¯æ˜¯å¦å˜åŒ–ï¼Œ 
![spark_succed2](./README.assets/spark_succed2.png)

å‘ç°spark å·²ç»æˆåŠŸæ¶ˆè´¹ï¼Œå¹¶ä¸”å°†æµä¼ ç»™äº†clickhouseã€‚

4. åœ¨clickhouseå®¢æˆ·ç«¯æŸ¥è¯¢ï¼Œæ‰“å¼€datagripæˆ‘ä»¬å‘ç°

![datagrip](./README.assets/datagrip.png)

é€šè¿‡æ—¶é—´æˆ‘ä»¬å‘ç°æœ€æ–°ä¸€æ‰¹æ•°æ®å·²ç»è¿›å…¥clickhouseï¼Œé¡¹ç›®æ•´ä½“æµç¨‹ç»“æŸã€‚ 

**ç¨‹åºè¿è¡Œæ•™å­¦è§†é¢‘æ¼”ç¤ºå°†åœ¨åç»­è¿­ä»£ç‰ˆæœ¬ä¸­æ”¾å…¥**

# 7.å¸¸è§é”™è¯¯



## è­¦å‘Š1 \## åœ¨æäº¤spark submit æ—¶å‡ºç°http5client çš„è­¦å‘Šï¼Œå¯¼å…¥mavenåŒ…å°±å¥½

```shell
spark-submit \
--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.clickhouse:clickhouse-jdbc:0.6.0,org.apache.httpcomponents.client5:httpclient5:5.3.1 \
kf_to_sstreaming.py 
```

## è­¦å‘Š2 \## clickhouse æäº¤å‡ºç°è­¦å‘Š

å‚è€ƒ https://github.com/ClickHouse/metabase-clickhouse-driver/blob/master/CHANGELOG.md æ˜¯å®˜æ–¹å®é”¤ ç³»ç»Ÿbugï¼Œä¸è¦ç´§

![clickhouse_warn](./README.assets/clickhouse_warn.png)

## è­¦å‘Š3 åœ¨å¯åŠ¨sparkstreamingåï¼Œæ¥å—æ¥è‡ªkafkaçš„æ¶ˆæ¯ï¼Œå‡ºç°KAFKA-1894

WARN KafkaDataConsumer: KafkaDataConsumer is not running in UninterruptibleThread. It may hang when KafkaDataConsumer's methods are interrupted because of KAFKA-1894

æ³¨æ„ï¼šå‡ºç°è¿™ç§æƒ…å†µ ä¸è¦ç®¡å®ƒï¼Œä¸å½±å“ä½¿ç”¨ï¼Œsparkstremingè·Ÿkafka apiéƒ¨åˆ†ä¸å…¼å®¹ æ²¡å…³ç³»
\# å¦‚æœä¸å–œæ¬¢warnï¼Œä¿®æ”¹log4j.propertiesæ–‡ä»¶ï¼Œå°†log4j.logger.org.apache.kafka.clients.NetworkClient=ERROR

## é”™è¯¯1 å…³äºå¯åŠ¨kafkaæ—¶çš„é—®é¢˜ï¼Œå…³äºbroker_id æˆ–è€… node é—®é¢˜

è§£å†³ï¼šæ³¨æ„ä¸€å®šè¦å…ˆå¯åŠ¨zookeeperï¼Œå¹¶ä¸”ç­‰ä¸€æ®µæ—¶é—´ï¼Œå†å¯åŠ¨kafkaå®¹å™¨
è‹¥æ˜¯ä¸è¡Œ è¯·åˆ é™¤zookeeper volumeä¸­ dataç›®å½•ä¸‹çš„ version-2 æ–‡ä»¶ï¼Œé‡å¯å®¹å™¨ï¼Œ è‹¥è¿˜æ˜¯ä¸ä¼šå¯è¯¢é—®chatgptï¼Œå„å¤§ç½‘ç«™ã€‚

## é”™è¯¯2 å…³äºå®¹å™¨é€šä¿¡é—®é¢˜

æœ‰äº›å°ä¼™ä¼´ï¼Œå®¹å™¨æ²¡æœ‰ä½¿ç”¨docker-composeå®‰è£…ï¼Œå¦‚æœé‡‡ç”¨ç‹¬ç«‹å®‰è£…ï¼Œè¯·è®©æ¯ä¸ªå®¹å™¨å¤„äºåŒä¸€ä¸ªç½‘ç»œä¸‹

ä½¿ç”¨ï¼š `docker network connect å®¹å™¨å ç½‘ç»œå` å³å¯ æ— éœ€é‡å¯å®¹å™¨

## æ•°æ®é‡‡é›†å¼‚å¸¸

è§£å†³ï¼šè¯·å»å„å¤§apiç½‘ç«™æŸ¥çœ‹æ˜¯å¦api å·²æ›´æ–°ï¼Œæˆ–è€…jsonç»“æ„å·²å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦å„ä½å°ä¼™ä¼´è‡ªè¡Œæµ‹è¯•ï¼Œå¯ä»¥æ‰“å¼€ä¸€ä¸ªipynbæ–‡ä»¶ è¿›è¡Œæ•°æ®ä¿å­˜ æµ‹è¯•ï¼Œ**æ³¨æ„ä¸è¦åœ¨è„šæœ¬æ–‡ä»¶ä¸Šæµ‹è¯•ï¼Œå› ä¸ºå„å¤§api è¯·æ±‚æ¬¡æ•°æœ‰é™ï¼Œè°¨æ…æ“ä½œ**

## å…¶ä»–é”™è¯¯

**å¦‚æœè¿˜æ˜¯å­˜åœ¨é”™è¯¯ï¼Œè¯·åœ¨github discussion ä¸Šç•™è¨€ï¼Œçœ‹åˆ°åæˆ‘ä¼šç¬¬ä¸€æ—¶é—´è§£ç­”ã€‚**



# 8.ç»“è¯­

ğŸ¾ğŸ¾ğŸ¾ç¥è´ºï¼ï¼ï¼ ä½ å·²ç»æˆåŠŸå®Œæˆäº†è¿™ä¸ªåŸºäºsparkè‡ªåŠ¨NLPæƒ…æ„Ÿåˆ†æçš„é¡¹ç›®ï¼Œç°åœ¨ä½ ç†Ÿæ‚‰äº†ä»0åˆ°1å¦‚ä½•æ­å»ºèµ·å¤§æ•°æ®æ¡†æ¶åŠ æ·±åº¦å­¦ä¹ çš„æµç¨‹ï¼Œåœ¨æ—¥åå­¦ä¹ ä¸­ï¼Œå¯ä»¥æŒ‰ç…§è¿™ä¸ªæ€è·¯è¿›è¡Œé¡¹ç›®æ„æ€ï¼Œéå¸¸æ„Ÿè°¢ä½ çš„é˜…è¯»ğŸ™. 
æˆ‘ä»¬åç»­åˆ†å¸ƒå¼é¡¹ç›®å†è§ğŸ‘‹ï¼ˆåˆ›ä½œä¸æ˜“ğŸ˜Šï¼Œå¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹äºä½ ä»¬æœ‰ä¸å°‘çš„çµæ„Ÿï¼Œå¸Œæœ›å¯ä»¥è´¡çŒ®ä¸€ä¸‹ä½ ä»¬çš„â­ï¸â­ï¸â­ï¸.ï¼‰

é¸£è°¢ @ç½‘å‹ spider å¯¹é¡¹ç›®æ”¹è¿›çš„æ„è§.

# 9.é™„å½•

- hugging_face æ¨¡å‹ä¸‹è½½é“¾æ¥ï¼š*https://huggingface.co/CHEN6688/DistillBert_for_sparkStreaming/tree/main*
- Transformer classification æ•™ç¨‹æ¥æºå·´é»è¨å…‹é›·å¤§å­¦ç ”ç©¶å‘˜marcevrardï¼š*https://github.com/marcevrard/nlp-with-transformers-book*
- æœ¬é¡¹ç›®finetuningæ¨¡å‹æ¥è‡ª hugging face ï¼š *https://huggingface.co/CHEN6688/DistillBert_for_sparkStreaming/tree/main*
- Distillbertæ¨¡å‹æ¨ªå‘å¯¹æ¯”åŸºçº¿æ¨¡å‹å‡†ç¡®ç‡ æ¥è‡ªå®éªŒï¼š*https://github.com/Michel-debug/NLP_Sentiment_analyse*å„å¤§æ–°é—»api æ¥æºrapideapi ï¼š*https://rapidapi.com/hub*
- å…¶ä»–api ï¼š *https://newsapi.org*
- sparkstreamingå‚è€ƒæ–‡æ¡£:*https://spark.apache.org/streaming/*
- Kafka producer api å‚è€ƒæ–‡æ¡£ï¼š*https://kafka.apache.org/documentation/#producerapi*

# æ³•å¾‹å£°æ˜

***Copyright Â© 2024 Junjie CHEN. All rights reserved. Unauthorized copying of this file, via any medium is strictly prohibited***

***Copyright Â© 2024 https://github.com/Michel-debug. All rights reserved. Unauthorized copying of this file, via any medium is strictly prohibited***

























